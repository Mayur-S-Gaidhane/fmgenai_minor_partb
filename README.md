# FMGenAI Minor par B —  (Q1, Q2 )


# FMGenAI Minor Part B — Ollama-only (Windows-friendly)

Runs locally with **Ollama** (no API keys, no cost).

## Quickstart (Windows + Conda + VS Code)

```bash
conda create -n fmgenai python=3.10 -y
conda activate fmgenai
pip install -r requirements.txt
```

### Provider options

```
Install Ollama (one-time): https://ollama.com/download

**Ollama (local, open-source):**
```bash
ollama pull mistral:7b-instruct
export PROVIDER=ollama
export OLLAMA_MODEL="mistral:7b-instruct"
# config.yaml -> model is use Ollama version is 0.12.3 .
bash run.sh
```

Outputs are written to `results/` and plots to `plots/`.

## Repo Layout

```
src/
  llm.py               # provider-agnostic LLM wrapper
  utils.py             # scoring & helpers
  evaluate_q1.py       # Multilingual & Code-Switch Stress Test
  evaluate_q2.py       # Robustness to Messy Inputs
  evaluate_q3.py       # Needle-in-a-Haystack
data/
  q1_prompts.csv       # 20 parallel items + code-switch
  q2_clean.csv         # filler text auto-generated by script
results/
  ...csv outputs...
plots/
  ...figures...
report/
  report_outline.md    # Outline for report PDF
README.md
config.yaml
run.sh
requirements.txt
LLM_Usage.md
DATA.md
```

## Reproducibility

- One-command run via `bash run.sh`
- CSVs with per-example predictions
- Minimal configs checked in
- Methods templates in `report/report_outline.md`

# session env (or use setx to make permanent)
$env:PROVIDER="ollama"
$env:OLLAMA_MODEL="mistral:7b-instruct"

# Q1 baseline (20 items × 4 conditions = 80 calls) it takes 40 min to run in my local system execution speed depend on GPU/CPU 
python -m src.evaluate_q1 --config config.yaml

# Q1 mitigation re-test (6 items/condition) it takes 15 min to run in my local system execution speed depend on GPU/CPU 
python -m src.evaluate_q1 --config config.yaml --mitigate

# Q2 full (50 clean + noisy variants (4 types × 2 levels × 50 each)= 450 calls ) it takes 60 min to run in my local system execution speed depend on GPU/CPU 
python -m src.evaluate_q2 --config config.yaml --intervention prompt

# quick test for Q2 ()
python -m src.evaluate_q2 --config config.yaml --intervention prompt --quick

# (optional alt) use preprocessing instead of prompt:
# python -m src.evaluate_q2 --config config.yaml --intervention preprocess

# full run Q1 and Q2 together :
.\run.bat

# Fluency analysis 
python fluency_sampler.py     --- Print out 10 rows per condition in the terminal to check fluency

python fluency_avg.py         --- This gives you a clean table output of Avg_fluency 

python test.py                --- To check what flency score read by pandas if fluency score none .

python fluency_probe_and_avg.py --- it will list out 6 columes value to in first 5 row in q1_results.csv

python merge_q1_tables.py     --- It is combine output table of q1_summary.csv and q1_fluency_summary.csv.